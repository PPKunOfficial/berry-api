# Berry API æ¥å£å‚è€ƒ

## ğŸ” è®¤è¯

æ‰€æœ‰éœ€è¦è®¤è¯çš„APIéƒ½ä½¿ç”¨Bearer Tokenè®¤è¯ï¼š

```http
Authorization: Bearer your-token-here
```

## ğŸ“‹ OpenAIå…¼å®¹æ¥å£

### POST /v1/chat/completions

åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚ï¼Œå®Œå…¨å…¼å®¹OpenAI APIã€‚

**è¯·æ±‚ä½“**ï¼š
```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  "stream": false,
  "max_tokens": 1000,
  "temperature": 0.7,
  "top_p": 1.0,
  "frequency_penalty": 0,
  "presence_penalty": 0,
  "stop": null
}
```

**å“åº”ï¼ˆéæµå¼ï¼‰**ï¼š
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-4",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

**å“åº”ï¼ˆæµå¼ï¼‰**ï¼š
```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### GET /v1/models

è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚

**å“åº”**ï¼š
```json
{
  "object": "list",
  "data": [
    {
      "id": "gpt-4",
      "object": "model",
      "created": 1677610602,
      "owned_by": "berry-api"
    },
    {
      "id": "gpt-3.5-turbo",
      "object": "model",
      "created": 1677610602,
      "owned_by": "berry-api"
    }
  ]
}
```

## ğŸ¥ å¥åº·æ£€æŸ¥æ¥å£

### GET /health

åŸºç¡€å¥åº·æ£€æŸ¥ï¼Œè¿”å›æœåŠ¡çŠ¶æ€ã€‚

**å“åº”**ï¼š
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

### GET /v1/health

OpenAIå…¼å®¹çš„å¥åº·æ£€æŸ¥æ¥å£ã€‚

**å“åº”**ï¼š
```json
{
  "status": "ok"
}
```

## ğŸ“Š æŒ‡æ ‡æ¥å£

### GET /metrics

è·å–è¯¦ç»†çš„æœåŠ¡æŒ‡æ ‡å’Œå¥åº·çŠ¶æ€ã€‚

**å“åº”**ï¼š
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "providers": {
    "openai": {
      "healthy": true,
      "last_check": "2024-01-15T10:29:45Z",
      "total_requests": 1250,
      "successful_requests": 1200,
      "failed_requests": 50,
      "average_latency_ms": 850,
      "models": {
        "gpt-4": {
          "healthy": true,
          "requests": 800,
          "errors": 20
        }
      }
    }
  },
  "models": {
    "gpt-4": {
      "total_requests": 800,
      "successful_requests": 780,
      "failed_requests": 20,
      "strategy": "weighted_failover"
    }
  },
  "load_balancer": {
    "total_selections": 5000,
    "strategy_distribution": {
      "weighted_failover": 3000,
      "smart_ai": 2000
    }
  },
  "static_files": {
    "total_files": 15,
    "total_size_bytes": 2048576
  }
}
```

### GET /prometheus

è·å–Prometheusæ ¼å¼çš„æŒ‡æ ‡æ•°æ®ã€‚

**å“åº”**ï¼š
```
# HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="POST",status="200",endpoint="/v1/chat/completions"} 1250

# HELP backend_health_status Backend health status (0=unhealthy, 1=healthy)
# TYPE backend_health_status gauge
backend_health_status{provider="openai",model="gpt-4"} 1

# HELP backend_latency_seconds Backend response latency
# TYPE backend_latency_seconds histogram
backend_latency_seconds_bucket{provider="openai",model="gpt-4",le="0.1"} 100
backend_latency_seconds_bucket{provider="openai",model="gpt-4",le="0.5"} 800
backend_latency_seconds_bucket{provider="openai",model="gpt-4",le="1.0"} 1200
backend_latency_seconds_bucket{provider="openai",model="gpt-4",le="+Inf"} 1250
```

## ğŸ›ï¸ ç®¡ç†æ¥å£

### GET /admin/model-weights

è·å–æ¨¡å‹æƒé‡ä¿¡æ¯ï¼ˆéœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰ã€‚

**å“åº”**ï¼š
```json
{
  "models": {
    "gpt-4": {
      "strategy": "weighted_failover",
      "backends": [
        {
          "provider": "openai",
          "model": "gpt-4",
          "weight": 0.7,
          "priority": 1,
          "healthy": true,
          "current_weight": 0.7
        },
        {
          "provider": "azure",
          "model": "gpt-4",
          "weight": 0.3,
          "priority": 2,
          "healthy": true,
          "current_weight": 0.3
        }
      ]
    }
  }
}
```

### GET /admin/backend-health

è·å–åç«¯å¥åº·çŠ¶æ€è¯¦æƒ…ï¼ˆéœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰ã€‚

**å“åº”**ï¼š
```json
{
  "backends": {
    "openai:gpt-4": {
      "healthy": true,
      "last_check": "2024-01-15T10:29:45Z",
      "consecutive_failures": 0,
      "total_requests": 800,
      "successful_requests": 780,
      "failed_requests": 20,
      "average_latency_ms": 850,
      "last_error": null
    },
    "azure:gpt-4": {
      "healthy": false,
      "last_check": "2024-01-15T10:29:30Z",
      "consecutive_failures": 3,
      "total_requests": 200,
      "successful_requests": 180,
      "failed_requests": 20,
      "average_latency_ms": 1200,
      "last_error": "Connection timeout"
    }
  }
}
```

### GET /admin/system-stats

è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ï¼ˆéœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰ã€‚

**å“åº”**ï¼š
```json
{
  "uptime_seconds": 86400,
  "total_requests": 5000,
  "requests_per_second": 10.5,
  "memory_usage_mb": 128,
  "active_connections": 25,
  "load_balancer_stats": {
    "total_selections": 5000,
    "average_selection_time_ms": 2.5,
    "cache_hit_rate": 0.95
  }
}
```

## ğŸ§  SmartAIæ¥å£

### GET /smart-ai/weights

è·å–SmartAIå…¨å±€æƒé‡ä¿¡æ¯ã€‚

**å“åº”**ï¼š
```json
{
  "smart_ai_enabled": true,
  "global_settings": {
    "initial_confidence": 0.8,
    "min_confidence": 0.05,
    "exploration_ratio": 0.2
  },
  "models": {
    "gpt-4": {
      "strategy": "smart_ai",
      "backends": [
        {
          "provider": "cheap_provider",
          "model": "gpt-3.5-turbo",
          "confidence": 0.75,
          "effective_weight": 0.85,
          "tags": []
        },
        {
          "provider": "premium_provider",
          "model": "gpt-4",
          "confidence": 0.90,
          "effective_weight": 0.45,
          "tags": ["premium"]
        }
      ]
    }
  }
}
```

### GET /smart-ai/models/{model}/weights

è·å–ç‰¹å®šæ¨¡å‹çš„SmartAIæƒé‡ä¿¡æ¯ã€‚

**å“åº”**ï¼š
```json
{
  "model": "gpt-4",
  "strategy": "smart_ai",
  "last_updated": "2024-01-15T10:30:00Z",
  "backends": [
    {
      "provider": "cheap_provider",
      "model": "gpt-3.5-turbo",
      "base_weight": 1.0,
      "confidence": 0.75,
      "stability_bonus": 1.1,
      "effective_weight": 0.825,
      "selection_probability": 0.65,
      "recent_requests": 150,
      "recent_successes": 145,
      "tags": []
    },
    {
      "provider": "premium_provider",
      "model": "gpt-4",
      "base_weight": 0.5,
      "confidence": 0.90,
      "stability_bonus": 1.0,
      "effective_weight": 0.45,
      "selection_probability": 0.35,
      "recent_requests": 80,
      "recent_successes": 78,
      "tags": ["premium"]
    }
  ]
}
```

## âŒ é”™è¯¯å“åº”

æ‰€æœ‰é”™è¯¯å“åº”éƒ½éµå¾ªç»Ÿä¸€æ ¼å¼ï¼š

```json
{
  "error": {
    "type": "invalid_token",
    "message": "The provided API key is invalid",
    "code": 401
  }
}
```

**å¸¸è§é”™è¯¯ç±»å‹**ï¼š
- `invalid_token` (401): æ— æ•ˆçš„è®¤è¯Token
- `model_access_denied` (403): æ¨¡å‹è®¿é—®è¢«æ‹’ç»
- `rate_limit_exceeded` (429): è¶…è¿‡é€Ÿç‡é™åˆ¶
- `service_unavailable` (503): æœåŠ¡ä¸å¯ç”¨
- `gateway_timeout` (504): ç½‘å…³è¶…æ—¶
- `internal_error` (500): å†…éƒ¨æœåŠ¡å™¨é”™è¯¯

## ğŸ“ è¯·æ±‚ç¤ºä¾‹

### cURLç¤ºä¾‹

```bash
# èŠå¤©å®Œæˆ
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-token" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# è·å–æ¨¡å‹åˆ—è¡¨
curl -H "Authorization: Bearer your-token" \
     http://localhost:3000/v1/models

# å¥åº·æ£€æŸ¥
curl http://localhost:3000/health
```

### Pythonç¤ºä¾‹

```python
import openai

client = openai.OpenAI(
    api_key="your-token",
    base_url="http://localhost:3000/v1"
)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### JavaScriptç¤ºä¾‹

```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'your-token',
  baseURL: 'http://localhost:3000/v1',
});

const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello!' }],
});
```
