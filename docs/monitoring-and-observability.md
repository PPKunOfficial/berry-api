# ğŸ“Š ç›‘æ§ä¸å¯è§‚æµ‹æ€§

Berry API æä¾›å®Œæ•´çš„å¯è§‚æµ‹æ€§æ”¯æŒï¼ŒåŒ…æ‹¬æŒ‡æ ‡æ”¶é›†ã€æ—¥å¿—è®°å½•å’Œå¥åº·ç›‘æ§ã€‚

### ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡

**HTTP è¯·æ±‚æŒ‡æ ‡**

-   `http_requests_total` - æ€»è¯·æ±‚æ•°ï¼ˆæŒ‰çŠ¶æ€ç ã€æ–¹æ³•ã€è·¯å¾„åˆ†ç±»ï¼‰
-   `http_request_duration_seconds` - è¯·æ±‚å»¶è¿Ÿåˆ†å¸ƒ
-   `http_requests_in_flight` - å½“å‰å¤„ç†ä¸­çš„è¯·æ±‚æ•°

**åç«¯å¥åº·æŒ‡æ ‡**

-   `backend_health_status` - åç«¯å¥åº·çŠ¶æ€ï¼ˆ0=ä¸å¥åº·ï¼Œ1=å¥åº·ï¼‰
-   `backend_request_count_total` - åç«¯è¯·æ±‚æ€»æ•°
-   `backend_error_count_total` - åç«¯é”™è¯¯æ€»æ•°
-   `backend_latency_seconds` - åç«¯å“åº”å»¶è¿Ÿ

**è´Ÿè½½å‡è¡¡æŒ‡æ ‡**

-   `load_balance_selections_total` - è´Ÿè½½å‡è¡¡é€‰æ‹©æ¬¡æ•°
-   `smart_ai_confidence_score` - SmartAIä¿¡å¿ƒåº¦åˆ†æ•°
-   `circuit_breaker_state` - ç†”æ–­å™¨çŠ¶æ€

### ğŸ“ˆ Prometheus é›†æˆ

**å¯ç”¨å¯è§‚æµ‹æ€§åŠŸèƒ½**

```bash
# ç¼–è¯‘æ—¶å¯ç”¨observabilityç‰¹æ€§
cargo build --release --features observability

# æˆ–åœ¨Cargo.tomlä¸­é…ç½®
[features]
default = ["observability"]
observability = ["prometheus", "axum-prometheus"]
```

**Prometheus é…ç½®**

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'berry-api'
    static_configs:
      - targets: ['localhost:3000']
    metrics_path: '/prometheus'
    scrape_interval: 10s
```

**Grafana ä»ªè¡¨æ¿**

åˆ›å»º Grafana ä»ªè¡¨æ¿ç›‘æ§å…³é”®æŒ‡æ ‡ï¼š

```json
{
  "dashboard": {
    "title": "Berry API Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ]
      },
      {
        "title": "Backend Health",
        "type": "stat",
        "targets": [
          {
            "expr": "backend_health_status",
            "legendFormat": "{{provider}}:{{model}}"
          }
        ]
      }
    ]
  }
}
```

### ğŸ“ æ—¥å¿—ç®¡ç†

**æ—¥å¿—çº§åˆ«é…ç½®**

```bash
# ç¯å¢ƒå˜é‡é…ç½®
export RUST_LOG=info                    # åŸºç¡€æ—¥å¿—
export RUST_LOG=debug                   # è°ƒè¯•æ—¥å¿—
export RUST_LOG=berry_api=debug         # ç‰¹å®šæ¨¡å—æ—¥å¿—
export RUST_LOG=trace                   # è¯¦ç»†è·Ÿè¸ªæ—¥å¿—
```

**ç»“æ„åŒ–æ—¥å¿—ç¤ºä¾‹**

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "target": "berry_api::loadbalance",
  "message": "Backend selected",
  "fields": {
    "provider": "openai",
    "model": "gpt-4",
    "strategy": "weighted_failover",
    "latency_ms": 850
  }
}
```

**æ—¥å¿—åˆ†æå‘½ä»¤**

```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" logs/berry-api.log | jq .

# ç›‘æ§å¥åº·æ£€æŸ¥
grep "health_check" logs/berry-api.log | tail -20

# åˆ†ææ€§èƒ½æŒ‡æ ‡
grep "latency" logs/berry-api.log | jq '.fields.latency_ms' | sort -n

# ç»Ÿè®¡è¯·æ±‚åˆ†å¸ƒ
grep "Backend selected" logs/berry-api.log | jq -r '.fields.provider' | sort | uniq -c
```

### ğŸš¨ å‘Šè­¦é…ç½®

**Prometheus å‘Šè­¦è§„åˆ™**

```yaml
# alerts.yml
groups:
  - name: berry-api
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"

      - alert: BackendDown
        expr: backend_health_status == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Backend {{ $labels.provider }}:{{ $labels.model }} is down"

      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
```

### ğŸ” å¥åº·æ£€æŸ¥ç›‘æ§

**å¥åº·æ£€æŸ¥ç«¯ç‚¹**

```bash
# åŸºç¡€å¥åº·æ£€æŸ¥
curl http://localhost:3000/health

# è¯¦ç»†å¥åº·çŠ¶æ€
curl http://localhost:3000/metrics | jq .

# ç‰¹å®šåç«¯å¥åº·çŠ¶æ€
curl http://localhost:3000/admin/backend-health
```

**å¥åº·çŠ¶æ€å“åº”ç¤ºä¾‹**

```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "providers": {
    "openai": {
      "healthy": true,
      "last_check": "2024-01-15T10:29:45Z",
      "total_requests": 1250,
      "successful_requests": 1200,
      "failed_requests": 50,
      "average_latency_ms": 850,
      "models": {
        "gpt-4": {
          "healthy": true,
          "requests": 800,
          "errors": 20
        }
      }
    }
  },
  "load_balancer": {
    "total_selections": 5000,
    "strategy_distribution": {
      "weighted_failover": 3000,
      "smart_ai": 2000
    }
  }
}
```
